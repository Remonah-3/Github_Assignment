{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfNqfSq/kkADJ9K1qtX8zF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Remonah-3/Github_Assignment/blob/master/Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XDNZ-Votu88V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    def __init__(self, n_in, n_out, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_in, n_out)\n",
        "        self.B = initializer.B(n_out)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = np.dot(X, self.W) + self.B\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dW = np.dot(self.X.T, dA)\n",
        "        dB = np.sum(dA, axis=0)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "        # Update weights\n",
        "        self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB)\n",
        "        return dZ\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializer:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_in, n_out):\n",
        "        return np.random.randn(n_in, n_out) * self.sigma\n",
        "\n",
        "    def B(self, n_out):\n",
        "        return np.zeros(n_out)\n"
      ],
      "metadata": {
        "id": "QuXld5N7vsv0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        W_new = W - self.lr * dW\n",
        "        B_new = B - self.lr * dB\n",
        "        return W_new, B_new\n"
      ],
      "metadata": {
        "id": "nSonDGbavvla"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.tanh(X)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, dA):\n",
        "        return dA * (1 - self.Z**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "        self.Z = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, Y):\n",
        "        # Cross-entropy loss derivative\n",
        "        batch_size = Y.shape[0]\n",
        "        return (self.Z - Y) / batch_size\n"
      ],
      "metadata": {
        "id": "hugBcR1Yvx_G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dZ = dA.copy()\n",
        "        dZ[self.X <= 0] = 0\n",
        "        return dZ\n"
      ],
      "metadata": {
        "id": "Jn72rHNlv0Kr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XavierInitializer:\n",
        "    def W(self, n_in, n_out):\n",
        "        sigma = 1 / np.sqrt(n_in)\n",
        "        return np.random.randn(n_in, n_out) * sigma\n",
        "    def B(self, n_out):\n",
        "        return np.zeros(n_out)\n",
        "\n",
        "class HeInitializer:\n",
        "    def W(self, n_in, n_out):\n",
        "        sigma = np.sqrt(2 / n_in)\n",
        "        return np.random.randn(n_in, n_out) * sigma\n",
        "    def B(self, n_out):\n",
        "        return np.zeros(n_out)\n"
      ],
      "metadata": {
        "id": "osWeOXT8v3QX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "    def __init__(self, lr, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.H_W = 0\n",
        "        self.H_B = 0\n",
        "\n",
        "    def update(self, W, B, dW, dB):\n",
        "        self.H_W += dW**2\n",
        "        self.H_B += dB**2\n",
        "        W_new = W - self.lr * dW / (np.sqrt(self.H_W) + self.epsilon)\n",
        "        B_new = B - self.lr * dB / (np.sqrt(self.H_B) + self.epsilon)\n",
        "        return W_new, B_new\n"
      ],
      "metadata": {
        "id": "6HJ7ANdQv5Y7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchDeepNeuralNetworkClassifier:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers  # list of (FC, activation) tuples\n",
        "\n",
        "    def forward(self, X):\n",
        "        for fc, act in self.layers:\n",
        "            X = fc.forward(X)\n",
        "            X = act.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, Y):\n",
        "        dA = None\n",
        "        for fc, act in reversed(self.layers):\n",
        "            if isinstance(act, Softmax):\n",
        "                dA = act.backward(Y)\n",
        "            else:\n",
        "                dA = act.backward(dA)\n",
        "            dA = fc.backward(dA)\n",
        "\n",
        "    def fit(self, X, Y, epochs=10):\n",
        "        for _ in range(epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(Y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n"
      ],
      "metadata": {
        "id": "EZGPBqqlv73Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy dataset\n",
        "X = np.random.randn(100, 784)\n",
        "Y = np.zeros((100, 10))\n",
        "Y[np.arange(100), np.random.randint(0,10,100)] = 1\n",
        "\n",
        "# Build network\n",
        "lr = 0.01\n",
        "sigma = 0.1\n",
        "optimizer = SGD(lr)\n",
        "layers = [\n",
        "    (FC(784, 128, SimpleInitializer(sigma), optimizer), Tanh()),\n",
        "    (FC(128, 64, SimpleInitializer(sigma), optimizer), ReLU()),\n",
        "    (FC(64, 10, SimpleInitializer(sigma), optimizer), Softmax())\n",
        "]\n",
        "\n",
        "model = ScratchDeepNeuralNetworkClassifier(layers)\n",
        "model.fit(X, Y, epochs=5)\n",
        "preds = model.predict(X)\n",
        "accuracy = np.mean(preds == np.argmax(Y, axis=1))\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4xLIvFpv-my",
        "outputId": "faacd252-3e18-4658-d0f2-7d19f4266b71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "    def __init__(self, n_in, n_out, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_in, n_out)\n",
        "        self.B = initializer.B(n_out)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.dot(X, self.W) + self.B\n",
        "\n",
        "    def backward(self, dA):\n",
        "        dW = np.dot(self.X.T, dA)\n",
        "        dB = np.sum(dA, axis=0)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "        self.W, self.B = self.optimizer.update(self.W, self.B, dW, dB)\n",
        "        return dZ\n",
        "\n",
        "# Initializers\n",
        "\n",
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_in, n_out):\n",
        "        return np.random.randn(n_in, n_out) * self.sigma\n",
        "    def B(self, n_out):\n",
        "        return np.zeros(n_out)\n",
        "\n",
        "class XavierInitializer:\n",
        "    def W(self, n_in, n_out):\n",
        "        sigma = 1 / np.sqrt(n_in)\n",
        "        return np.random.randn(n_in, n_out) * sigma\n",
        "    def B(self, n_out):\n",
        "        return np.zeros(n_out)\n",
        "\n",
        "class HeInitializer:\n",
        "    def W(self, n_in, n_out):\n",
        "        sigma = np.sqrt(2 / n_in)\n",
        "        return np.random.randn(n_in, n_out) * sigma\n",
        "    def B(self, n_out):\n",
        "        return np.zeros(n_out)\n",
        "\n",
        "# Optimizers\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, W, B, dW, dB):\n",
        "        return W - self.lr * dW, B - self.lr * dB\n",
        "\n",
        "class AdaGrad:\n",
        "    def __init__(self, lr, epsilon=1e-7):\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.H_W = 0\n",
        "        self.H_B = 0\n",
        "    def update(self, W, B, dW, dB):\n",
        "        self.H_W += dW**2\n",
        "        self.H_B += dB**2\n",
        "        W_new = W - self.lr * dW / (np.sqrt(self.H_W) + self.epsilon)\n",
        "        B_new = B - self.lr * dB / (np.sqrt(self.H_B) + self.epsilon)\n",
        "        return W_new, B_new\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.tanh(X)\n",
        "        return self.Z\n",
        "    def backward(self, dA):\n",
        "        return dA * (1 - self.Z**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "        self.Z = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        return (self.Z - Y) / Y.shape[0]\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.maximum(0, X)\n",
        "    def backward(self, dA):\n",
        "        dZ = dA.copy()\n",
        "        dZ[self.X <= 0] = 0\n",
        "        return dZ\n",
        "\n",
        "# Deep Neural Network Class\n",
        "\n",
        "class ScratchDeepNeuralNetworkClassifier:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers  # list of (FC, activation) tuples\n",
        "\n",
        "    def forward(self, X):\n",
        "        for fc, act in self.layers:\n",
        "            X = fc.forward(X)\n",
        "            X = act.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, Y):\n",
        "        dA = None\n",
        "        for fc, act in reversed(self.layers):\n",
        "            if isinstance(act, Softmax):\n",
        "                dA = act.backward(Y)\n",
        "            else:\n",
        "                dA = act.backward(dA)\n",
        "            dA = fc.backward(dA)\n",
        "\n",
        "    def fit(self, X, Y, epochs=10):\n",
        "        for _ in range(epochs):\n",
        "            self.forward(X)\n",
        "            self.backward(Y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n",
        "\n",
        "# Training\n",
        "\n",
        "X = np.random.randn(100, 784)\n",
        "Y = np.zeros((100, 10))\n",
        "Y[np.arange(100), np.random.randint(0, 10, 100)] = 1\n",
        "\n",
        "# Build network\n",
        "lr = 0.01\n",
        "sigma = 0.1\n",
        "optimizer = SGD(lr)\n",
        "layers = [\n",
        "    (FC(784, 128, SimpleInitializer(sigma), optimizer), Tanh()),\n",
        "    (FC(128, 64, SimpleInitializer(sigma), optimizer), ReLU()),\n",
        "    (FC(64, 10, SimpleInitializer(sigma), optimizer), Softmax())\n",
        "]\n",
        "\n",
        "model = ScratchDeepNeuralNetworkClassifier(layers)\n",
        "model.fit(X, Y, epochs=5)\n",
        "preds = model.predict(X)\n",
        "accuracy = np.mean(preds == np.argmax(Y, axis=1))\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeJhjxF4wBss",
        "outputId": "4780f868-0a72-4ba9-82a2-04a35f052156"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.13\n"
          ]
        }
      ]
    }
  ]
}