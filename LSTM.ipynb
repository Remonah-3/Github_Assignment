{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmeuyc/p7tphtnfMurdByY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Remonah-3/Github_Assignment/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apKZwng_IlIc",
        "outputId": "f3efea87-9cb7-4be6-91f7-624a55e0c59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Training SimpleRNN...\n",
            "Epoch 1/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - accuracy: 0.5197 - loss: 0.7025 - val_accuracy: 0.5882 - val_loss: 0.6533\n",
            "Epoch 2/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 73ms/step - accuracy: 0.6699 - loss: 0.5996 - val_accuracy: 0.6881 - val_loss: 0.5818\n",
            "Epoch 3/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 86ms/step - accuracy: 0.7564 - loss: 0.4922 - val_accuracy: 0.6596 - val_loss: 0.6215\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.6591 - loss: 0.6256\n",
            "SimpleRNN Test accuracy: 0.6596\n",
            "\n",
            "Training GRU...\n",
            "Epoch 1/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 249ms/step - accuracy: 0.6864 - loss: 0.5602 - val_accuracy: 0.8376 - val_loss: 0.3679\n",
            "Epoch 2/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 250ms/step - accuracy: 0.8914 - loss: 0.2697 - val_accuracy: 0.8526 - val_loss: 0.3423\n",
            "Epoch 3/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 277ms/step - accuracy: 0.9455 - loss: 0.1499 - val_accuracy: 0.8332 - val_loss: 0.4600\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 31ms/step - accuracy: 0.8333 - loss: 0.4661\n",
            "GRU Test accuracy: 0.8332\n",
            "\n",
            "Training LSTM...\n",
            "Epoch 1/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 283ms/step - accuracy: 0.7066 - loss: 0.5446 - val_accuracy: 0.8241 - val_loss: 0.3960\n",
            "Epoch 2/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 301ms/step - accuracy: 0.8787 - loss: 0.3005 - val_accuracy: 0.8315 - val_loss: 0.3754\n",
            "Epoch 3/3\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 301ms/step - accuracy: 0.9202 - loss: 0.2034 - val_accuracy: 0.8330 - val_loss: 0.4164\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.8302 - loss: 0.4256\n",
            "LSTM Test accuracy: 0.8330\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import ConvLSTM2D, Conv3D, BatchNormalization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "# Load IMDB data\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "def train_rnn(model_type='SimpleRNN'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, 128))\n",
        "\n",
        "    if model_type == 'SimpleRNN':\n",
        "        model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'LSTM':\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    print(f\"\\nTraining {model_type}...\")\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=3,  # Keep low for quick testing\n",
        "              validation_data=(x_test, y_test))\n",
        "\n",
        "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "    print(f\"{model_type} Test accuracy: {acc:.4f}\")\n",
        "\n",
        "train_rnn('SimpleRNN')\n",
        "train_rnn('GRU')\n",
        "train_rnn('LSTM')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D, Conv3D, BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "# Build ConvLSTM model\n",
        "seq = Sequential()\n",
        "seq.add(ConvLSTM2D(filters=16, kernel_size=(3, 3),\n",
        "                   input_shape=(None, 40, 40, 1),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "seq.add(ConvLSTM2D(filters=16, kernel_size=(3, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
        "               activation='sigmoid', padding='same'))\n",
        "\n",
        "seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
        "\n",
        "# Generate artificial movie data\n",
        "def generate_movies(n_samples=200, n_frames=10):\n",
        "    row, col = 40, 40\n",
        "    X = np.zeros((n_samples, n_frames, row, col, 1))\n",
        "    Y = np.zeros((n_samples, n_frames, row, col, 1))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        xstart, ystart = np.random.randint(0, 35, 2)\n",
        "        directionx, directiony = np.random.randint(-1, 2, 2)\n",
        "        for t in range(n_frames):\n",
        "            x_shift = xstart + directionx * t\n",
        "            y_shift = ystart + directiony * t\n",
        "            X[i, t, x_shift:x_shift+5, y_shift:y_shift+5, 0] = 1\n",
        "            Y[i, t, x_shift+directionx:x_shift+directionx+5,\n",
        "                  y_shift+directiony:y_shift+directiony+5, 0] = 1\n",
        "    return X, Y\n",
        "\n",
        "# Generate data\n",
        "noisy_movies, shifted_movies = generate_movies()\n",
        "\n",
        "# Train model\n",
        "seq.fit(noisy_movies, shifted_movies, batch_size=10, epochs=5, validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv-dE2yFIx8Q",
        "outputId": "b199bab4-0f7f-4a77-b25e-9cd153da3334"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - loss: 0.7856 - val_loss: 0.6968\n",
            "Epoch 2/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - loss: 0.7817 - val_loss: 0.6998\n",
            "Epoch 3/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 0.7803 - val_loss: 0.7028\n",
            "Epoch 4/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - loss: 0.7765 - val_loss: 0.7058\n",
            "Epoch 5/5\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - loss: 0.7789 - val_loss: 0.7090\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fed5f978470>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import ConvLSTM2D, Conv3D, BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "# IMDB RNNs\n",
        "max_features = 20000\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "# Load IMDB data\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "def train_rnn(model_type='SimpleRNN', epochs=3):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, 128))\n",
        "\n",
        "    if model_type == 'SimpleRNN':\n",
        "        model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    elif model_type == 'LSTM':\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    print(f\"\\nTraining {model_type}...\")\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(x_test, y_test), verbose=2)\n",
        "\n",
        "    _, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
        "    print(f\"{model_type} Test Accuracy: {acc:.4f}\")\n",
        "    return acc\n",
        "\n",
        "# Train all three and store accuracies\n",
        "results = {}\n",
        "for rnn_type in ['SimpleRNN', 'GRU', 'LSTM']:\n",
        "    results[rnn_type] = train_rnn(rnn_type, epochs=3)\n",
        "\n",
        "# Print comparison table\n",
        "print(\"\\n--- Accuracy Comparison ---\")\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# ConvLSTM2D\n",
        "# Generate simple moving squares dataset\n",
        "def generate_movies(n_samples=200, n_frames=10):\n",
        "    row, col = 40, 40\n",
        "    X = np.zeros((n_samples, n_frames, row, col, 1))\n",
        "    Y = np.zeros((n_samples, n_frames, row, col, 1))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        xstart, ystart = np.random.randint(0, 35, 2)\n",
        "        directionx, directiony = np.random.randint(-1, 2, 2)\n",
        "        for t in range(n_frames):\n",
        "            x_shift = xstart + directionx * t\n",
        "            y_shift = ystart + directiony * t\n",
        "            X[i, t, x_shift:x_shift+5, y_shift:y_shift+5, 0] = 1\n",
        "            Y[i, t, x_shift+directionx:x_shift+directionx+5,\n",
        "                  y_shift+directiony:y_shift+directiony+5, 0] = 1\n",
        "    return X, Y\n",
        "\n",
        "noisy_movies, shifted_movies = generate_movies()\n",
        "\n",
        "# Build ConvLSTM2D model\n",
        "seq = Sequential()\n",
        "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3),\n",
        "                   input_shape=(None, 40, 40, 1),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "seq.add(ConvLSTM2D(filters=16, kernel_size=(3,3),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "seq.add(Conv3D(filters=1, kernel_size=(3,3,3), activation='sigmoid', padding='same'))\n",
        "\n",
        "seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
        "\n",
        "seq.fit(noisy_movies, shifted_movies, batch_size=10, epochs=5, validation_split=0.1, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSWDe9rHKBau",
        "outputId": "06a4621e-3b86-4f18-c81c-eac1b8695ae6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training SimpleRNN...\n",
            "Epoch 1/3\n",
            "782/782 - 59s - 75ms/step - accuracy: 0.5399 - loss: 0.6860 - val_accuracy: 0.6145 - val_loss: 0.6396\n",
            "Epoch 2/3\n",
            "782/782 - 80s - 102ms/step - accuracy: 0.7227 - loss: 0.5453 - val_accuracy: 0.7918 - val_loss: 0.4817\n",
            "Epoch 3/3\n",
            "782/782 - 83s - 107ms/step - accuracy: 0.7916 - loss: 0.4574 - val_accuracy: 0.7408 - val_loss: 0.5293\n",
            "SimpleRNN Test Accuracy: 0.7408\n",
            "\n",
            "Training GRU...\n",
            "Epoch 1/3\n",
            "782/782 - 208s - 266ms/step - accuracy: 0.7718 - loss: 0.4725 - val_accuracy: 0.8332 - val_loss: 0.3798\n",
            "Epoch 2/3\n",
            "782/782 - 186s - 237ms/step - accuracy: 0.8903 - loss: 0.2696 - val_accuracy: 0.8486 - val_loss: 0.3479\n",
            "Epoch 3/3\n",
            "782/782 - 201s - 257ms/step - accuracy: 0.9417 - loss: 0.1572 - val_accuracy: 0.8352 - val_loss: 0.4159\n",
            "GRU Test Accuracy: 0.8352\n",
            "\n",
            "Training LSTM...\n",
            "Epoch 1/3\n",
            "782/782 - 222s - 284ms/step - accuracy: 0.7784 - loss: 0.4616 - val_accuracy: 0.8354 - val_loss: 0.3786\n",
            "Epoch 2/3\n",
            "782/782 - 248s - 317ms/step - accuracy: 0.8823 - loss: 0.2912 - val_accuracy: 0.8404 - val_loss: 0.3751\n",
            "Epoch 3/3\n",
            "782/782 - 205s - 262ms/step - accuracy: 0.9194 - loss: 0.2060 - val_accuracy: 0.8276 - val_loss: 0.4797\n",
            "LSTM Test Accuracy: 0.8276\n",
            "\n",
            "--- Accuracy Comparison ---\n",
            "SimpleRNN: 0.7408\n",
            "GRU: 0.8352\n",
            "LSTM: 0.8276\n",
            "Epoch 1/5\n",
            "18/18 - 46s - 3s/step - loss: 0.7731 - val_loss: 0.6931\n",
            "Epoch 2/5\n",
            "18/18 - 81s - 4s/step - loss: 0.7704 - val_loss: 0.6932\n",
            "Epoch 3/5\n",
            "18/18 - 41s - 2s/step - loss: 0.7676 - val_loss: 0.6933\n",
            "Epoch 4/5\n",
            "18/18 - 41s - 2s/step - loss: 0.7650 - val_loss: 0.6935\n",
            "Epoch 5/5\n",
            "18/18 - 41s - 2s/step - loss: 0.7623 - val_loss: 0.6938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fed32135eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "max_words = 10000  # top 10,000 words\n",
        "maxlen = 100       # max sequence length\n",
        "batch_size = 32\n",
        "\n",
        "# Load Reuters dataset\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words)\n",
        "\n",
        "# Pad sequences\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Convert labels to one-hot (multi-class classification)\n",
        "num_classes = max(y_train) + 1\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f'Training samples: {x_train.shape}, Test samples: {x_test.shape}, Classes: {num_classes}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFWiJ8I0KCiX",
        "outputId": "5b9c5e43-4295-49a1-e241-cb5871ba03b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training samples: (8982, 100), Test samples: (2246, 100), Classes: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=maxlen))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))  # multi-class output\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 5\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    verbose=2)\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print(f\"Reuters LSTM Test Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr8ckz5DM6G2",
        "outputId": "7998a458-f67f-4749-dd69-06f606601b4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "281/281 - 87s - 309ms/step - accuracy: 0.4466 - loss: 2.1559 - val_accuracy: 0.5227 - val_loss: 1.8018\n",
            "Epoch 2/5\n",
            "281/281 - 80s - 283ms/step - accuracy: 0.5577 - loss: 1.6907 - val_accuracy: 0.5984 - val_loss: 1.6378\n",
            "Epoch 3/5\n",
            "281/281 - 82s - 292ms/step - accuracy: 0.6199 - loss: 1.4649 - val_accuracy: 0.6273 - val_loss: 1.5086\n",
            "Epoch 4/5\n",
            "281/281 - 82s - 293ms/step - accuracy: 0.6691 - loss: 1.2752 - val_accuracy: 0.6318 - val_loss: 1.4342\n",
            "Epoch 5/5\n",
            "281/281 - 79s - 280ms/step - accuracy: 0.7185 - loss: 1.0799 - val_accuracy: 0.6518 - val_loss: 1.4105\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.6565 - loss: 1.3814\n",
            "Reuters LSTM Test Accuracy: 0.6518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **RNN**\n",
        "The RNN class is the base class for all recurrent layers in Keras. It defines how sequences are processed over time. Most of the time, we do not use it directly because it is low-level. Instead, we use higher-level layers like SimpleRNN, GRU, or LSTM, which are easier to work with. You would only use RNN directly if you want to create a custom RNN layer.\n",
        "\n",
        "##### **SimpleRNNCell**\n",
        "The SimpleRNNCell class represents a single time step of a SimpleRNN. It calculates the next hidden state for one step in the sequence. In practice, we rarely use it directly because the SimpleRNN layer already handles the full sequence automatically. It is mainly for custom RNN designs where you need fine control.\n",
        "\n",
        "##### **GRUCell**\n",
        "The GRUCell class represents a single time step of a GRU (Gated Recurrent Unit). Like SimpleRNNCell, it computes the hidden state for one step only. Most of the time, we just use the GRU layer, which handles the whole sequence. GRUCell is used when you want custom sequence processing.\n",
        "\n",
        "##### **LSTMCell**\n",
        "The LSTMCell class represents a single time step of an LSTM (Long Short-Term Memory). It calculates the hidden and cell states for one step. Usually, we use the LSTM layer instead of LSTMCell because it automatically processes the full sequence. This cell is useful if you want custom LSTM behavior.\n",
        "\n",
        "##### **StackedRNNCells**\n",
        "The StackedRNNCells class allows you to combine multiple RNN cells into one layer. It can be used to create a multi-layer RNN manually. In practice, most people just stack RNN layers sequentially using SimpleRNN, GRU, or LSTM, so this class is rarely needed.\n",
        "\n",
        "##### **CuDNNGRU**\n",
        "The CuDNNGRU is a GPU-optimized GRU layer. It is much faster than a regular GRU when training on a GPU. However, it cannot run on a CPU and has fewer options for dropout and activation functions. It is useful when training large models on big datasets.\n",
        "\n",
        "##### **CuDNNLSTM**\n",
        "The CuDNNLSTM is a GPU-optimized LSTM layer. It trains much faster on a GPU than a normal LSTM. Like CuDNNGRU, it cannot run on a CPU and has limited configuration options. It is very useful for large LSTM models where speed is important."
      ],
      "metadata": {
        "id": "bwiHoHJ3M8vY"
      }
    }
  ]
}