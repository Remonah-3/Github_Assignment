{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu8DrK9JhPZILuLmOA9xoc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Remonah-3/Github_Assignment/blob/master/Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L92ZXeL4HCbq",
        "outputId": "ab24c951-879e-491f-924a-0e5025fe381a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4473 - loss: 0.8295\n",
            "Epoch 2/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4411 - loss: 0.7640\n",
            "Epoch 3/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5036 - loss: 0.7155\n",
            "Epoch 4/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5016 - loss: 0.6960\n",
            "Epoch 5/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5930 - loss: 0.6411\n",
            "Epoch 6/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6375 - loss: 0.6183\n",
            "Epoch 7/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8295 - loss: 0.5903\n",
            "Epoch 8/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8279 - loss: 0.5681\n",
            "Epoch 9/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9277 - loss: 0.5429\n",
            "Epoch 10/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9149 - loss: 0.5297\n",
            "Epoch 11/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8772 - loss: 0.5193\n",
            "Epoch 12/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9239 - loss: 0.5017\n",
            "Epoch 13/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9684 - loss: 0.4467\n",
            "Epoch 14/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9790 - loss: 0.4358\n",
            "Epoch 15/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9199 - loss: 0.4513\n",
            "Epoch 16/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9326 - loss: 0.4128\n",
            "Epoch 17/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9278 - loss: 0.4139\n",
            "Epoch 18/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9753 - loss: 0.3742\n",
            "Epoch 19/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9677 - loss: 0.3534\n",
            "Epoch 20/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9404 - loss: 0.3589\n",
            "Epoch 21/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9261 - loss: 0.3866\n",
            "Epoch 22/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9472 - loss: 0.3352\n",
            "Epoch 23/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9391 - loss: 0.3326 \n",
            "Epoch 24/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9534 - loss: 0.2720\n",
            "Epoch 25/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9594 - loss: 0.2829\n",
            "Epoch 26/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9184 - loss: 0.3098\n",
            "Epoch 27/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9795 - loss: 0.2471\n",
            "Epoch 28/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9822 - loss: 0.2695\n",
            "Epoch 29/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9570 - loss: 0.2620\n",
            "Epoch 30/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9588 - loss: 0.2415\n",
            "Epoch 31/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9312 - loss: 0.2414\n",
            "Epoch 32/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9641 - loss: 0.2214\n",
            "Epoch 33/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9717 - loss: 0.2089\n",
            "Epoch 34/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9860 - loss: 0.1949\n",
            "Epoch 35/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9441 - loss: 0.2441\n",
            "Epoch 36/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9365 - loss: 0.2500\n",
            "Epoch 37/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9656 - loss: 0.1868 \n",
            "Epoch 38/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9698 - loss: 0.1839\n",
            "Epoch 39/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9502 - loss: 0.2050\n",
            "Epoch 40/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9674 - loss: 0.1836  \n",
            "Epoch 41/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9661 - loss: 0.1685 \n",
            "Epoch 42/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9515 - loss: 0.1685 \n",
            "Epoch 43/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9801 - loss: 0.1534 \n",
            "Epoch 44/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.1715 \n",
            "Epoch 45/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9368 - loss: 0.1767 \n",
            "Epoch 46/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9632 - loss: 0.1602 \n",
            "Epoch 47/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9721 - loss: 0.1432 \n",
            "Epoch 48/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9479 - loss: 0.1428 \n",
            "Epoch 49/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9510 - loss: 0.1752 \n",
            "Epoch 50/50\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1507 \n",
            "\n",
            "Test Accuracy: 0.8999999761581421\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "Actual: 1, Predicted: 1, Prob: 0.51\n",
            "Actual: 1, Predicted: 1, Prob: 0.77\n",
            "Actual: 1, Predicted: 1, Prob: 0.96\n",
            "Actual: 0, Predicted: 0, Prob: 0.01\n",
            "Actual: 0, Predicted: 0, Prob: 0.04\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "data = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# Keep only two species: Iris-versicolor and Iris-virginica\n",
        "data = data[data[\"Species\"].isin([\"Iris-versicolor\", \"Iris-virginica\"])]\n",
        "\n",
        "# Prepare features and labels\n",
        "\n",
        "X = data[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = data[\"Species\"]\n",
        "\n",
        "# Convert labels to 0 and 1\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)  # versicolor=0, virginica=1\n",
        "\n",
        "# Split into train and test sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale (normalize) the features\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build a simple model (low-level tf.keras)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)),  # hidden layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # output layer (binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=4, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"\\nTest Accuracy:\", accuracy)\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predicted_labels = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Show some results\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_test[i]}, Predicted: {predicted_labels[i][0]:.0f}, Prob: {predictions[i][0]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and prepare the dataset\n",
        "\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "\n",
        "# Features and labels\n",
        "X = df[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
        "y = np.where(df[\"Species\"] == \"Iris-virginica\", 1, 0).reshape(-1, 1)\n",
        "\n",
        "# Split data → train / val / test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Normalize features AND convert to float32\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_val = scaler.transform(X_val).astype(np.float32)\n",
        "X_test = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "# Convert labels to float32\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_val = y_val.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Define hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_classes = 1\n",
        "\n",
        "# Build the model (manual weights, like old TensorFlow)\n",
        "\n",
        "initializer = tf.initializers.GlorotUniform(seed=0)\n",
        "\n",
        "W1 = tf.Variable(initializer([n_input, n_hidden1]))\n",
        "b1 = tf.Variable(tf.zeros([n_hidden1]))\n",
        "W2 = tf.Variable(initializer([n_hidden1, n_hidden2]))\n",
        "b2 = tf.Variable(tf.zeros([n_hidden2]))\n",
        "W3 = tf.Variable(initializer([n_hidden2, n_classes]))\n",
        "b3 = tf.Variable(tf.zeros([n_classes]))\n",
        "\n",
        "# Forward pass\n",
        "def forward(x):\n",
        "    layer1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
        "    logits = tf.matmul(layer2, W3) + b3\n",
        "    return logits\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Mini-batch generator\n",
        "\n",
        "def get_mini_batches(X, y, batch_size):\n",
        "    n = X.shape[0]\n",
        "    idx = np.random.permutation(n)\n",
        "    X, y = X[idx], y[idx]\n",
        "    for i in range(0, n, batch_size):\n",
        "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "# Training loop\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in get_mini_batches(X_train, y_train, batch_size):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = forward(X_batch)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "        optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "    # Validation loss and accuracy\n",
        "    val_logits = forward(X_val)\n",
        "    val_loss = loss_fn(y_val, val_logits).numpy()\n",
        "    val_pred = tf.sigmoid(val_logits) > 0.5\n",
        "    val_acc = np.mean((val_pred.numpy() == y_val).astype(np.float32))\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | loss: {total_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.3f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "\n",
        "test_logits = forward(X_test)\n",
        "test_pred = tf.sigmoid(test_logits) > 0.5\n",
        "test_acc = np.mean((test_pred.numpy() == y_test).astype(np.float32))\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CmH11ZGHy_a",
        "outputId": "a80a9810-fd25-4beb-fd44-603926f173db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss: 4.8609 | val_loss: 0.6380 | val_acc: 0.562\n",
            "Epoch 002 | loss: 4.1942 | val_loss: 0.5180 | val_acc: 0.938\n",
            "Epoch 003 | loss: 3.6296 | val_loss: 0.4343 | val_acc: 0.938\n",
            "Epoch 004 | loss: 3.0115 | val_loss: 0.3595 | val_acc: 1.000\n",
            "Epoch 005 | loss: 2.7071 | val_loss: 0.2959 | val_acc: 1.000\n",
            "Epoch 006 | loss: 2.3597 | val_loss: 0.2433 | val_acc: 1.000\n",
            "Epoch 007 | loss: 2.0981 | val_loss: 0.1993 | val_acc: 1.000\n",
            "Epoch 008 | loss: 1.7855 | val_loss: 0.1629 | val_acc: 1.000\n",
            "Epoch 009 | loss: 1.4939 | val_loss: 0.1364 | val_acc: 1.000\n",
            "Epoch 010 | loss: 1.3234 | val_loss: 0.1142 | val_acc: 1.000\n",
            "Epoch 011 | loss: 1.1835 | val_loss: 0.0974 | val_acc: 1.000\n",
            "Epoch 012 | loss: 1.0255 | val_loss: 0.0826 | val_acc: 1.000\n",
            "Epoch 013 | loss: 0.8650 | val_loss: 0.0713 | val_acc: 1.000\n",
            "Epoch 014 | loss: 0.7501 | val_loss: 0.0637 | val_acc: 1.000\n",
            "Epoch 015 | loss: 0.6846 | val_loss: 0.0575 | val_acc: 1.000\n",
            "Epoch 016 | loss: 0.6226 | val_loss: 0.0518 | val_acc: 1.000\n",
            "Epoch 017 | loss: 0.6119 | val_loss: 0.0485 | val_acc: 1.000\n",
            "Epoch 018 | loss: 0.5185 | val_loss: 0.0451 | val_acc: 1.000\n",
            "Epoch 019 | loss: 0.4686 | val_loss: 0.0426 | val_acc: 1.000\n",
            "Epoch 020 | loss: 0.4657 | val_loss: 0.0405 | val_acc: 1.000\n",
            "Epoch 021 | loss: 0.3986 | val_loss: 0.0386 | val_acc: 1.000\n",
            "Epoch 022 | loss: 0.5298 | val_loss: 0.0375 | val_acc: 1.000\n",
            "Epoch 023 | loss: 0.3437 | val_loss: 0.0349 | val_acc: 1.000\n",
            "Epoch 024 | loss: 0.3255 | val_loss: 0.0347 | val_acc: 1.000\n",
            "Epoch 025 | loss: 0.2973 | val_loss: 0.0331 | val_acc: 1.000\n",
            "Epoch 026 | loss: 0.2766 | val_loss: 0.0322 | val_acc: 1.000\n",
            "Epoch 027 | loss: 0.3153 | val_loss: 0.0316 | val_acc: 1.000\n",
            "Epoch 028 | loss: 0.3105 | val_loss: 0.0353 | val_acc: 1.000\n",
            "Epoch 029 | loss: 0.2528 | val_loss: 0.0361 | val_acc: 1.000\n",
            "Epoch 030 | loss: 0.2163 | val_loss: 0.0349 | val_acc: 1.000\n",
            "Epoch 031 | loss: 0.1984 | val_loss: 0.0332 | val_acc: 1.000\n",
            "Epoch 032 | loss: 0.1923 | val_loss: 0.0313 | val_acc: 1.000\n",
            "Epoch 033 | loss: 0.1816 | val_loss: 0.0308 | val_acc: 1.000\n",
            "Epoch 034 | loss: 0.1689 | val_loss: 0.0289 | val_acc: 1.000\n",
            "Epoch 035 | loss: 0.1595 | val_loss: 0.0278 | val_acc: 1.000\n",
            "Epoch 036 | loss: 0.1537 | val_loss: 0.0277 | val_acc: 1.000\n",
            "Epoch 037 | loss: 0.1473 | val_loss: 0.0283 | val_acc: 1.000\n",
            "Epoch 038 | loss: 0.1351 | val_loss: 0.0279 | val_acc: 1.000\n",
            "Epoch 039 | loss: 0.1308 | val_loss: 0.0277 | val_acc: 1.000\n",
            "Epoch 040 | loss: 0.1800 | val_loss: 0.0272 | val_acc: 1.000\n",
            "Epoch 041 | loss: 0.1158 | val_loss: 0.0275 | val_acc: 1.000\n",
            "Epoch 042 | loss: 0.1115 | val_loss: 0.0266 | val_acc: 1.000\n",
            "Epoch 043 | loss: 0.1304 | val_loss: 0.0271 | val_acc: 1.000\n",
            "Epoch 044 | loss: 0.1204 | val_loss: 0.0245 | val_acc: 1.000\n",
            "Epoch 045 | loss: 0.1345 | val_loss: 0.0242 | val_acc: 1.000\n",
            "Epoch 046 | loss: 0.0946 | val_loss: 0.0241 | val_acc: 1.000\n",
            "Epoch 047 | loss: 0.0865 | val_loss: 0.0230 | val_acc: 1.000\n",
            "Epoch 048 | loss: 0.1198 | val_loss: 0.0226 | val_acc: 1.000\n",
            "Epoch 049 | loss: 0.0788 | val_loss: 0.0241 | val_acc: 1.000\n",
            "Epoch 050 | loss: 0.0741 | val_loss: 0.0248 | val_acc: 1.000\n",
            "Epoch 051 | loss: 0.0732 | val_loss: 0.0261 | val_acc: 1.000\n",
            "Epoch 052 | loss: 0.0779 | val_loss: 0.0257 | val_acc: 1.000\n",
            "Epoch 053 | loss: 0.0670 | val_loss: 0.0235 | val_acc: 1.000\n",
            "Epoch 054 | loss: 0.0623 | val_loss: 0.0231 | val_acc: 1.000\n",
            "Epoch 055 | loss: 0.0599 | val_loss: 0.0222 | val_acc: 1.000\n",
            "Epoch 056 | loss: 0.0579 | val_loss: 0.0222 | val_acc: 1.000\n",
            "Epoch 057 | loss: 0.0556 | val_loss: 0.0224 | val_acc: 1.000\n",
            "Epoch 058 | loss: 0.0817 | val_loss: 0.0225 | val_acc: 1.000\n",
            "Epoch 059 | loss: 0.0663 | val_loss: 0.0229 | val_acc: 1.000\n",
            "Epoch 060 | loss: 0.0640 | val_loss: 0.0219 | val_acc: 1.000\n",
            "Epoch 061 | loss: 0.0471 | val_loss: 0.0197 | val_acc: 1.000\n",
            "Epoch 062 | loss: 0.0620 | val_loss: 0.0193 | val_acc: 1.000\n",
            "Epoch 063 | loss: 0.0440 | val_loss: 0.0193 | val_acc: 1.000\n",
            "Epoch 064 | loss: 0.0424 | val_loss: 0.0191 | val_acc: 1.000\n",
            "Epoch 065 | loss: 0.0422 | val_loss: 0.0200 | val_acc: 1.000\n",
            "Epoch 066 | loss: 0.0393 | val_loss: 0.0197 | val_acc: 1.000\n",
            "Epoch 067 | loss: 0.0381 | val_loss: 0.0201 | val_acc: 1.000\n",
            "Epoch 068 | loss: 0.0367 | val_loss: 0.0196 | val_acc: 1.000\n",
            "Epoch 069 | loss: 0.0462 | val_loss: 0.0197 | val_acc: 1.000\n",
            "Epoch 070 | loss: 0.0344 | val_loss: 0.0184 | val_acc: 1.000\n",
            "Epoch 071 | loss: 0.0339 | val_loss: 0.0177 | val_acc: 1.000\n",
            "Epoch 072 | loss: 0.0325 | val_loss: 0.0184 | val_acc: 1.000\n",
            "Epoch 073 | loss: 0.0321 | val_loss: 0.0186 | val_acc: 1.000\n",
            "Epoch 074 | loss: 0.0317 | val_loss: 0.0183 | val_acc: 1.000\n",
            "Epoch 075 | loss: 0.0392 | val_loss: 0.0178 | val_acc: 1.000\n",
            "Epoch 076 | loss: 0.0286 | val_loss: 0.0180 | val_acc: 1.000\n",
            "Epoch 077 | loss: 0.0277 | val_loss: 0.0183 | val_acc: 1.000\n",
            "Epoch 078 | loss: 0.0421 | val_loss: 0.0182 | val_acc: 1.000\n",
            "Epoch 079 | loss: 0.0258 | val_loss: 0.0182 | val_acc: 1.000\n",
            "Epoch 080 | loss: 0.0250 | val_loss: 0.0173 | val_acc: 1.000\n",
            "Epoch 081 | loss: 0.0240 | val_loss: 0.0175 | val_acc: 1.000\n",
            "Epoch 082 | loss: 0.0234 | val_loss: 0.0176 | val_acc: 1.000\n",
            "Epoch 083 | loss: 0.0254 | val_loss: 0.0174 | val_acc: 1.000\n",
            "Epoch 084 | loss: 0.0223 | val_loss: 0.0169 | val_acc: 1.000\n",
            "Epoch 085 | loss: 0.0236 | val_loss: 0.0170 | val_acc: 1.000\n",
            "Epoch 086 | loss: 0.0208 | val_loss: 0.0172 | val_acc: 1.000\n",
            "Epoch 087 | loss: 0.0258 | val_loss: 0.0169 | val_acc: 1.000\n",
            "Epoch 088 | loss: 0.0198 | val_loss: 0.0162 | val_acc: 1.000\n",
            "Epoch 089 | loss: 0.0211 | val_loss: 0.0160 | val_acc: 1.000\n",
            "Epoch 090 | loss: 0.0236 | val_loss: 0.0156 | val_acc: 1.000\n",
            "Epoch 091 | loss: 0.0180 | val_loss: 0.0153 | val_acc: 1.000\n",
            "Epoch 092 | loss: 0.0177 | val_loss: 0.0155 | val_acc: 1.000\n",
            "Epoch 093 | loss: 0.0196 | val_loss: 0.0159 | val_acc: 1.000\n",
            "Epoch 094 | loss: 0.0167 | val_loss: 0.0153 | val_acc: 1.000\n",
            "Epoch 095 | loss: 0.0209 | val_loss: 0.0152 | val_acc: 1.000\n",
            "Epoch 096 | loss: 0.0157 | val_loss: 0.0157 | val_acc: 1.000\n",
            "Epoch 097 | loss: 0.0154 | val_loss: 0.0162 | val_acc: 1.000\n",
            "Epoch 098 | loss: 0.0154 | val_loss: 0.0162 | val_acc: 1.000\n",
            "Epoch 099 | loss: 0.0205 | val_loss: 0.0159 | val_acc: 1.000\n",
            "Epoch 100 | loss: 0.0147 | val_loss: 0.0141 | val_acc: 1.000\n",
            "\n",
            "Test Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load and prepare the dataset\n",
        "\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# Features\n",
        "X = df[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values.astype(np.float32)\n",
        "\n",
        "# Labels (one-hot encode)\n",
        "species = df[\"Species\"].values.reshape(-1,1)\n",
        "encoder = OneHotEncoder(sparse_output=False)  # updated for modern scikit-learn\n",
        "y = encoder.fit_transform(species).astype(np.float32)  # shape (n_samples, 3)\n",
        "\n",
        "# Split into train / val / test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
        "X_val = scaler.transform(X_val).astype(np.float32)\n",
        "X_test = scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_classes = 3\n",
        "\n",
        "# Build the model (manual weights)\n",
        "\n",
        "initializer = tf.initializers.GlorotUniform(seed=0)\n",
        "\n",
        "W1 = tf.Variable(initializer([n_input, n_hidden1]))\n",
        "b1 = tf.Variable(tf.zeros([n_hidden1]))\n",
        "W2 = tf.Variable(initializer([n_hidden1, n_hidden2]))\n",
        "b2 = tf.Variable(tf.zeros([n_hidden2]))\n",
        "W3 = tf.Variable(initializer([n_hidden2, n_classes]))\n",
        "b3 = tf.Variable(tf.zeros([n_classes]))\n",
        "\n",
        "# Forward pass\n",
        "def forward(x):\n",
        "    layer1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
        "    logits = tf.matmul(layer2, W3) + b3\n",
        "    return logits\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Mini-batch generator\n",
        "def get_mini_batches(X, y, batch_size):\n",
        "    n = X.shape[0]\n",
        "    idx = np.random.permutation(n)\n",
        "    X, y = X[idx], y[idx]\n",
        "    for i in range(0, n, batch_size):\n",
        "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in get_mini_batches(X_train, y_train, batch_size):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = forward(X_batch)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "        optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "    # Validation\n",
        "    val_logits = forward(X_val)\n",
        "    val_loss = loss_fn(y_val, val_logits).numpy()\n",
        "    val_pred = tf.argmax(tf.nn.softmax(val_logits), axis=1)\n",
        "    val_true = tf.argmax(y_val, axis=1)\n",
        "    val_acc = np.mean((val_pred.numpy() == val_true.numpy()).astype(np.float32))\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | loss: {total_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.3f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_logits = forward(X_test)\n",
        "test_pred = tf.argmax(tf.nn.softmax(test_logits), axis=1)\n",
        "test_true = tf.argmax(y_test, axis=1)\n",
        "test_acc = np.mean((test_pred.numpy() == test_true.numpy()).astype(np.float32))\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n",
        "\n",
        "# Example predictions\n",
        "for i in range(5):\n",
        "    pred_class = encoder.categories_[0][test_pred[i].numpy()]\n",
        "    true_class = encoder.categories_[0][test_true[i].numpy()]\n",
        "    print(f\"Actual: {true_class}, Predicted: {pred_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wWcZlUOJPF7",
        "outputId": "19d5eb93-87ef-442b-89b6-0d026242d955"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss: 9.8335 | val_loss: 0.8280 | val_acc: 0.792\n",
            "Epoch 002 | loss: 7.4073 | val_loss: 0.6334 | val_acc: 0.792\n",
            "Epoch 003 | loss: 5.6026 | val_loss: 0.5173 | val_acc: 0.792\n",
            "Epoch 004 | loss: 4.5305 | val_loss: 0.4565 | val_acc: 0.792\n",
            "Epoch 005 | loss: 3.8793 | val_loss: 0.4174 | val_acc: 0.792\n",
            "Epoch 006 | loss: 3.4566 | val_loss: 0.3938 | val_acc: 0.792\n",
            "Epoch 007 | loss: 2.9897 | val_loss: 0.3651 | val_acc: 0.792\n",
            "Epoch 008 | loss: 2.6293 | val_loss: 0.3558 | val_acc: 0.792\n",
            "Epoch 009 | loss: 2.4700 | val_loss: 0.3616 | val_acc: 0.792\n",
            "Epoch 010 | loss: 2.1746 | val_loss: 0.3531 | val_acc: 0.792\n",
            "Epoch 011 | loss: 1.9632 | val_loss: 0.3540 | val_acc: 0.792\n",
            "Epoch 012 | loss: 1.8859 | val_loss: 0.3616 | val_acc: 0.792\n",
            "Epoch 013 | loss: 1.6855 | val_loss: 0.3446 | val_acc: 0.792\n",
            "Epoch 014 | loss: 1.5167 | val_loss: 0.3588 | val_acc: 0.792\n",
            "Epoch 015 | loss: 1.4488 | val_loss: 0.3595 | val_acc: 0.792\n",
            "Epoch 016 | loss: 1.3093 | val_loss: 0.3439 | val_acc: 0.792\n",
            "Epoch 017 | loss: 1.2045 | val_loss: 0.3527 | val_acc: 0.792\n",
            "Epoch 018 | loss: 1.1620 | val_loss: 0.3733 | val_acc: 0.792\n",
            "Epoch 019 | loss: 1.0438 | val_loss: 0.3446 | val_acc: 0.792\n",
            "Epoch 020 | loss: 1.0693 | val_loss: 0.3411 | val_acc: 0.792\n",
            "Epoch 021 | loss: 0.9309 | val_loss: 0.3713 | val_acc: 0.792\n",
            "Epoch 022 | loss: 0.9139 | val_loss: 0.3446 | val_acc: 0.833\n",
            "Epoch 023 | loss: 0.8830 | val_loss: 0.3412 | val_acc: 0.833\n",
            "Epoch 024 | loss: 0.8005 | val_loss: 0.3338 | val_acc: 0.833\n",
            "Epoch 025 | loss: 0.8218 | val_loss: 0.3254 | val_acc: 0.833\n",
            "Epoch 026 | loss: 0.7132 | val_loss: 0.3388 | val_acc: 0.833\n",
            "Epoch 027 | loss: 0.7167 | val_loss: 0.3438 | val_acc: 0.833\n",
            "Epoch 028 | loss: 0.6647 | val_loss: 0.3087 | val_acc: 0.833\n",
            "Epoch 029 | loss: 0.6521 | val_loss: 0.3058 | val_acc: 0.833\n",
            "Epoch 030 | loss: 0.6689 | val_loss: 0.3146 | val_acc: 0.833\n",
            "Epoch 031 | loss: 0.6201 | val_loss: 0.3278 | val_acc: 0.833\n",
            "Epoch 032 | loss: 0.6688 | val_loss: 0.2916 | val_acc: 0.875\n",
            "Epoch 033 | loss: 0.6079 | val_loss: 0.3389 | val_acc: 0.833\n",
            "Epoch 034 | loss: 0.5614 | val_loss: 0.3106 | val_acc: 0.875\n",
            "Epoch 035 | loss: 0.5207 | val_loss: 0.3022 | val_acc: 0.875\n",
            "Epoch 036 | loss: 0.5224 | val_loss: 0.2968 | val_acc: 0.875\n",
            "Epoch 037 | loss: 0.4846 | val_loss: 0.2944 | val_acc: 0.875\n",
            "Epoch 038 | loss: 0.5566 | val_loss: 0.2977 | val_acc: 0.875\n",
            "Epoch 039 | loss: 0.4483 | val_loss: 0.2495 | val_acc: 0.917\n",
            "Epoch 040 | loss: 0.5134 | val_loss: 0.2762 | val_acc: 0.917\n",
            "Epoch 041 | loss: 0.4802 | val_loss: 0.2587 | val_acc: 0.917\n",
            "Epoch 042 | loss: 0.4753 | val_loss: 0.2842 | val_acc: 0.917\n",
            "Epoch 043 | loss: 0.4090 | val_loss: 0.2883 | val_acc: 0.917\n",
            "Epoch 044 | loss: 0.3900 | val_loss: 0.2813 | val_acc: 0.917\n",
            "Epoch 045 | loss: 0.3854 | val_loss: 0.2706 | val_acc: 0.917\n",
            "Epoch 046 | loss: 0.3959 | val_loss: 0.2880 | val_acc: 0.917\n",
            "Epoch 047 | loss: 0.3738 | val_loss: 0.2482 | val_acc: 0.917\n",
            "Epoch 048 | loss: 0.4582 | val_loss: 0.2710 | val_acc: 0.917\n",
            "Epoch 049 | loss: 0.3434 | val_loss: 0.2610 | val_acc: 0.917\n",
            "Epoch 050 | loss: 0.3470 | val_loss: 0.2537 | val_acc: 0.917\n",
            "Epoch 051 | loss: 0.3436 | val_loss: 0.2722 | val_acc: 0.917\n",
            "Epoch 052 | loss: 0.3767 | val_loss: 0.2883 | val_acc: 0.917\n",
            "Epoch 053 | loss: 0.3310 | val_loss: 0.2337 | val_acc: 0.917\n",
            "Epoch 054 | loss: 0.3547 | val_loss: 0.2422 | val_acc: 0.917\n",
            "Epoch 055 | loss: 0.3637 | val_loss: 0.2966 | val_acc: 0.917\n",
            "Epoch 056 | loss: 0.4259 | val_loss: 0.2376 | val_acc: 0.917\n",
            "Epoch 057 | loss: 0.3643 | val_loss: 0.3014 | val_acc: 0.917\n",
            "Epoch 058 | loss: 0.2928 | val_loss: 0.2510 | val_acc: 0.917\n",
            "Epoch 059 | loss: 0.3113 | val_loss: 0.2433 | val_acc: 0.917\n",
            "Epoch 060 | loss: 0.2868 | val_loss: 0.2802 | val_acc: 0.917\n",
            "Epoch 061 | loss: 0.2934 | val_loss: 0.2480 | val_acc: 0.917\n",
            "Epoch 062 | loss: 0.3156 | val_loss: 0.2764 | val_acc: 0.917\n",
            "Epoch 063 | loss: 0.3241 | val_loss: 0.2694 | val_acc: 0.917\n",
            "Epoch 064 | loss: 0.2431 | val_loss: 0.2198 | val_acc: 0.917\n",
            "Epoch 065 | loss: 0.3092 | val_loss: 0.2392 | val_acc: 0.917\n",
            "Epoch 066 | loss: 0.2632 | val_loss: 0.2935 | val_acc: 0.917\n",
            "Epoch 067 | loss: 0.2669 | val_loss: 0.2795 | val_acc: 0.917\n",
            "Epoch 068 | loss: 0.2280 | val_loss: 0.2421 | val_acc: 0.917\n",
            "Epoch 069 | loss: 0.2416 | val_loss: 0.2554 | val_acc: 0.917\n",
            "Epoch 070 | loss: 0.2410 | val_loss: 0.2626 | val_acc: 0.917\n",
            "Epoch 071 | loss: 0.2133 | val_loss: 0.2583 | val_acc: 0.917\n",
            "Epoch 072 | loss: 0.2128 | val_loss: 0.2603 | val_acc: 0.917\n",
            "Epoch 073 | loss: 0.2053 | val_loss: 0.2643 | val_acc: 0.917\n",
            "Epoch 074 | loss: 0.2186 | val_loss: 0.2615 | val_acc: 0.917\n",
            "Epoch 075 | loss: 0.2017 | val_loss: 0.2798 | val_acc: 0.917\n",
            "Epoch 076 | loss: 0.2005 | val_loss: 0.2707 | val_acc: 0.917\n",
            "Epoch 077 | loss: 0.2185 | val_loss: 0.2509 | val_acc: 0.917\n",
            "Epoch 078 | loss: 0.2137 | val_loss: 0.2756 | val_acc: 0.917\n",
            "Epoch 079 | loss: 0.1834 | val_loss: 0.2661 | val_acc: 0.917\n",
            "Epoch 080 | loss: 0.1844 | val_loss: 0.2497 | val_acc: 0.917\n",
            "Epoch 081 | loss: 0.1808 | val_loss: 0.2656 | val_acc: 0.917\n",
            "Epoch 082 | loss: 0.1710 | val_loss: 0.2647 | val_acc: 0.917\n",
            "Epoch 083 | loss: 0.1692 | val_loss: 0.2755 | val_acc: 0.917\n",
            "Epoch 084 | loss: 0.1933 | val_loss: 0.2823 | val_acc: 0.917\n",
            "Epoch 085 | loss: 0.1524 | val_loss: 0.2578 | val_acc: 0.917\n",
            "Epoch 086 | loss: 0.1680 | val_loss: 0.2590 | val_acc: 0.917\n",
            "Epoch 087 | loss: 0.1650 | val_loss: 0.2718 | val_acc: 0.917\n",
            "Epoch 088 | loss: 0.1861 | val_loss: 0.2878 | val_acc: 0.917\n",
            "Epoch 089 | loss: 0.1571 | val_loss: 0.2767 | val_acc: 0.917\n",
            "Epoch 090 | loss: 0.1580 | val_loss: 0.2631 | val_acc: 0.917\n",
            "Epoch 091 | loss: 0.1472 | val_loss: 0.2786 | val_acc: 0.917\n",
            "Epoch 092 | loss: 0.1548 | val_loss: 0.2886 | val_acc: 0.917\n",
            "Epoch 093 | loss: 0.1429 | val_loss: 0.2866 | val_acc: 0.917\n",
            "Epoch 094 | loss: 0.1475 | val_loss: 0.2805 | val_acc: 0.917\n",
            "Epoch 095 | loss: 0.1312 | val_loss: 0.2907 | val_acc: 0.917\n",
            "Epoch 096 | loss: 0.1405 | val_loss: 0.2773 | val_acc: 0.917\n",
            "Epoch 097 | loss: 0.1310 | val_loss: 0.2903 | val_acc: 0.917\n",
            "Epoch 098 | loss: 0.1283 | val_loss: 0.2817 | val_acc: 0.917\n",
            "Epoch 099 | loss: 0.1257 | val_loss: 0.2836 | val_acc: 0.917\n",
            "Epoch 100 | loss: 0.1423 | val_loss: 0.2660 | val_acc: 0.917\n",
            "\n",
            "Test Accuracy: 1.0\n",
            "Actual: Iris-virginica, Predicted: Iris-virginica\n",
            "Actual: Iris-versicolor, Predicted: Iris-versicolor\n",
            "Actual: Iris-setosa, Predicted: Iris-setosa\n",
            "Actual: Iris-virginica, Predicted: Iris-virginica\n",
            "Actual: Iris-setosa, Predicted: Iris-setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and prepare the dataset\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Features\n",
        "X = df[[\"GrLivArea\", \"YearBuilt\"]].values.astype(np.float32)\n",
        "y = df[\"SalePrice\"].values.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "# Split into train / val / test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Normalize features\n",
        "scaler_X = StandardScaler()\n",
        "X_train = scaler_X.fit_transform(X_train).astype(np.float32)\n",
        "X_val = scaler_X.transform(X_val).astype(np.float32)\n",
        "X_test = scaler_X.transform(X_test).astype(np.float32)\n",
        "\n",
        "# Normalize target (optional but stabilizes training)\n",
        "y_mean = np.mean(y_train)\n",
        "y_std = np.std(y_train)\n",
        "y_train_norm = (y_train - y_mean) / y_std\n",
        "y_val_norm = (y_val - y_mean) / y_std\n",
        "y_test_norm = (y_test - y_mean) / y_std\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 200\n",
        "n_input = X_train.shape[1]\n",
        "n_hidden1 = 64\n",
        "n_hidden2 = 128\n",
        "n_output = 1  # regression\n",
        "\n",
        "# Build the model (manual weights)\n",
        "initializer = tf.initializers.GlorotUniform(seed=0)\n",
        "\n",
        "W1 = tf.Variable(initializer([n_input, n_hidden1]))\n",
        "b1 = tf.Variable(tf.zeros([n_hidden1]))\n",
        "W2 = tf.Variable(initializer([n_hidden1, n_hidden2]))\n",
        "b2 = tf.Variable(tf.zeros([n_hidden2]))\n",
        "W3 = tf.Variable(initializer([n_hidden2, n_output]))\n",
        "b3 = tf.Variable(tf.zeros([n_output]))\n",
        "\n",
        "# Forward pass\n",
        "def forward(x):\n",
        "    layer1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
        "    output = tf.matmul(layer2, W3) + b3  # linear activation for regression\n",
        "    return output\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Mini-batch generator\n",
        "def get_mini_batches(X, y, batch_size):\n",
        "    n = X.shape[0]\n",
        "    idx = np.random.permutation(n)\n",
        "    X, y = X[idx], y[idx]\n",
        "    for i in range(0, n, batch_size):\n",
        "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in get_mini_batches(X_train, y_train_norm, batch_size):\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred = forward(X_batch)\n",
        "            loss = loss_fn(y_batch, pred)\n",
        "        grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "        optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "    # Validation\n",
        "    val_pred = forward(X_val)\n",
        "    val_loss = loss_fn(y_val_norm, val_pred).numpy()\n",
        "    print(f\"Epoch {epoch+1:03d} | train_loss: {total_loss:.4f} | val_loss: {val_loss:.4f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "\n",
        "test_pred_norm = forward(X_test).numpy()\n",
        "# Convert back to original SalePrice scale\n",
        "test_pred = test_pred_norm * y_std + y_mean\n",
        "mse_test = np.mean((test_pred - y_test)**2)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "print(\"\\nTest RMSE:\", rmse_test)\n",
        "\n",
        "# Example predictions\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_test[i,0]:.0f}, Predicted: {test_pred[i,0]:.0f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF1l_aqyJQmK",
        "outputId": "5b6edc20-ed36-4b70-ca83-94f0aabd9263"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss: 17.2441 | val_loss: 0.2714\n",
            "Epoch 002 | train_loss: 9.9104 | val_loss: 0.2418\n",
            "Epoch 003 | train_loss: 9.4951 | val_loss: 0.2338\n",
            "Epoch 004 | train_loss: 11.1838 | val_loss: 0.2286\n",
            "Epoch 005 | train_loss: 9.2567 | val_loss: 0.2276\n",
            "Epoch 006 | train_loss: 9.6881 | val_loss: 0.2382\n",
            "Epoch 007 | train_loss: 9.1055 | val_loss: 0.2209\n",
            "Epoch 008 | train_loss: 9.1691 | val_loss: 0.2317\n",
            "Epoch 009 | train_loss: 9.2402 | val_loss: 0.2264\n",
            "Epoch 010 | train_loss: 8.8600 | val_loss: 0.2210\n",
            "Epoch 011 | train_loss: 8.8703 | val_loss: 0.2313\n",
            "Epoch 012 | train_loss: 8.7467 | val_loss: 0.2291\n",
            "Epoch 013 | train_loss: 8.9018 | val_loss: 0.2219\n",
            "Epoch 014 | train_loss: 8.7012 | val_loss: 0.2242\n",
            "Epoch 015 | train_loss: 8.6603 | val_loss: 0.2294\n",
            "Epoch 016 | train_loss: 8.8943 | val_loss: 0.2213\n",
            "Epoch 017 | train_loss: 8.6429 | val_loss: 0.2287\n",
            "Epoch 018 | train_loss: 8.7208 | val_loss: 0.2302\n",
            "Epoch 019 | train_loss: 8.7161 | val_loss: 0.2236\n",
            "Epoch 020 | train_loss: 8.6659 | val_loss: 0.2259\n",
            "Epoch 021 | train_loss: 9.0181 | val_loss: 0.2199\n",
            "Epoch 022 | train_loss: 8.7097 | val_loss: 0.2236\n",
            "Epoch 023 | train_loss: 8.8916 | val_loss: 0.2201\n",
            "Epoch 024 | train_loss: 8.9202 | val_loss: 0.2209\n",
            "Epoch 025 | train_loss: 9.1117 | val_loss: 0.2229\n",
            "Epoch 026 | train_loss: 8.5408 | val_loss: 0.2171\n",
            "Epoch 027 | train_loss: 10.4370 | val_loss: 0.2195\n",
            "Epoch 028 | train_loss: 9.9142 | val_loss: 0.2170\n",
            "Epoch 029 | train_loss: 8.8237 | val_loss: 0.2239\n",
            "Epoch 030 | train_loss: 8.6701 | val_loss: 0.2203\n",
            "Epoch 031 | train_loss: 8.7024 | val_loss: 0.2168\n",
            "Epoch 032 | train_loss: 8.5642 | val_loss: 0.2271\n",
            "Epoch 033 | train_loss: 8.5476 | val_loss: 0.2313\n",
            "Epoch 034 | train_loss: 8.4690 | val_loss: 0.2330\n",
            "Epoch 035 | train_loss: 8.9054 | val_loss: 0.2193\n",
            "Epoch 036 | train_loss: 8.5340 | val_loss: 0.2234\n",
            "Epoch 037 | train_loss: 8.4904 | val_loss: 0.2233\n",
            "Epoch 038 | train_loss: 8.3479 | val_loss: 0.2226\n",
            "Epoch 039 | train_loss: 8.4071 | val_loss: 0.2179\n",
            "Epoch 040 | train_loss: 8.5123 | val_loss: 0.2132\n",
            "Epoch 041 | train_loss: 8.4436 | val_loss: 0.2250\n",
            "Epoch 042 | train_loss: 8.5566 | val_loss: 0.2256\n",
            "Epoch 043 | train_loss: 8.5668 | val_loss: 0.2263\n",
            "Epoch 044 | train_loss: 8.5983 | val_loss: 0.2142\n",
            "Epoch 045 | train_loss: 8.2242 | val_loss: 0.2208\n",
            "Epoch 046 | train_loss: 8.2675 | val_loss: 0.2256\n",
            "Epoch 047 | train_loss: 8.6172 | val_loss: 0.2172\n",
            "Epoch 048 | train_loss: 8.3872 | val_loss: 0.2147\n",
            "Epoch 049 | train_loss: 8.3440 | val_loss: 0.2183\n",
            "Epoch 050 | train_loss: 8.2365 | val_loss: 0.2236\n",
            "Epoch 051 | train_loss: 8.2902 | val_loss: 0.2136\n",
            "Epoch 052 | train_loss: 8.5232 | val_loss: 0.2363\n",
            "Epoch 053 | train_loss: 8.8440 | val_loss: 0.2202\n",
            "Epoch 054 | train_loss: 8.3970 | val_loss: 0.2185\n",
            "Epoch 055 | train_loss: 8.4373 | val_loss: 0.2213\n",
            "Epoch 056 | train_loss: 8.4343 | val_loss: 0.2167\n",
            "Epoch 057 | train_loss: 8.2420 | val_loss: 0.2139\n",
            "Epoch 058 | train_loss: 8.7938 | val_loss: 0.2142\n",
            "Epoch 059 | train_loss: 8.3905 | val_loss: 0.2178\n",
            "Epoch 060 | train_loss: 8.3489 | val_loss: 0.2194\n",
            "Epoch 061 | train_loss: 8.3896 | val_loss: 0.2222\n",
            "Epoch 062 | train_loss: 8.4124 | val_loss: 0.2160\n",
            "Epoch 063 | train_loss: 8.2061 | val_loss: 0.2114\n",
            "Epoch 064 | train_loss: 8.6443 | val_loss: 0.2107\n",
            "Epoch 065 | train_loss: 8.2990 | val_loss: 0.2140\n",
            "Epoch 066 | train_loss: 8.3362 | val_loss: 0.2146\n",
            "Epoch 067 | train_loss: 8.3591 | val_loss: 0.2107\n",
            "Epoch 068 | train_loss: 8.1873 | val_loss: 0.2216\n",
            "Epoch 069 | train_loss: 8.5003 | val_loss: 0.2188\n",
            "Epoch 070 | train_loss: 8.0041 | val_loss: 0.2231\n",
            "Epoch 071 | train_loss: 8.5527 | val_loss: 0.2299\n",
            "Epoch 072 | train_loss: 8.0262 | val_loss: 0.2291\n",
            "Epoch 073 | train_loss: 8.4310 | val_loss: 0.2229\n",
            "Epoch 074 | train_loss: 8.1174 | val_loss: 0.2135\n",
            "Epoch 075 | train_loss: 8.2580 | val_loss: 0.2122\n",
            "Epoch 076 | train_loss: 8.3157 | val_loss: 0.2189\n",
            "Epoch 077 | train_loss: 8.1006 | val_loss: 0.2083\n",
            "Epoch 078 | train_loss: 8.3103 | val_loss: 0.2138\n",
            "Epoch 079 | train_loss: 8.0897 | val_loss: 0.2115\n",
            "Epoch 080 | train_loss: 8.2876 | val_loss: 0.2071\n",
            "Epoch 081 | train_loss: 8.2069 | val_loss: 0.2129\n",
            "Epoch 082 | train_loss: 7.9473 | val_loss: 0.2157\n",
            "Epoch 083 | train_loss: 8.0546 | val_loss: 0.2225\n",
            "Epoch 084 | train_loss: 8.1286 | val_loss: 0.2104\n",
            "Epoch 085 | train_loss: 8.1150 | val_loss: 0.2154\n",
            "Epoch 086 | train_loss: 8.4740 | val_loss: 0.2205\n",
            "Epoch 087 | train_loss: 8.3785 | val_loss: 0.2182\n",
            "Epoch 088 | train_loss: 8.1184 | val_loss: 0.2150\n",
            "Epoch 089 | train_loss: 8.0510 | val_loss: 0.2249\n",
            "Epoch 090 | train_loss: 7.9970 | val_loss: 0.2100\n",
            "Epoch 091 | train_loss: 7.9956 | val_loss: 0.2132\n",
            "Epoch 092 | train_loss: 7.8709 | val_loss: 0.2213\n",
            "Epoch 093 | train_loss: 8.0508 | val_loss: 0.2138\n",
            "Epoch 094 | train_loss: 8.3425 | val_loss: 0.2210\n",
            "Epoch 095 | train_loss: 8.2817 | val_loss: 0.2087\n",
            "Epoch 096 | train_loss: 7.9684 | val_loss: 0.2092\n",
            "Epoch 097 | train_loss: 7.9670 | val_loss: 0.2091\n",
            "Epoch 098 | train_loss: 8.1342 | val_loss: 0.2124\n",
            "Epoch 099 | train_loss: 8.3243 | val_loss: 0.2186\n",
            "Epoch 100 | train_loss: 8.0365 | val_loss: 0.2180\n",
            "Epoch 101 | train_loss: 8.0003 | val_loss: 0.2206\n",
            "Epoch 102 | train_loss: 8.0422 | val_loss: 0.2221\n",
            "Epoch 103 | train_loss: 9.0652 | val_loss: 0.2220\n",
            "Epoch 104 | train_loss: 8.1330 | val_loss: 0.2128\n",
            "Epoch 105 | train_loss: 8.0756 | val_loss: 0.2215\n",
            "Epoch 106 | train_loss: 7.9869 | val_loss: 0.2152\n",
            "Epoch 107 | train_loss: 8.0348 | val_loss: 0.2178\n",
            "Epoch 108 | train_loss: 8.3439 | val_loss: 0.2135\n",
            "Epoch 109 | train_loss: 8.0889 | val_loss: 0.2071\n",
            "Epoch 110 | train_loss: 8.2187 | val_loss: 0.2065\n",
            "Epoch 111 | train_loss: 8.0185 | val_loss: 0.2066\n",
            "Epoch 112 | train_loss: 7.8518 | val_loss: 0.2110\n",
            "Epoch 113 | train_loss: 7.9234 | val_loss: 0.2119\n",
            "Epoch 114 | train_loss: 7.9384 | val_loss: 0.2093\n",
            "Epoch 115 | train_loss: 8.3015 | val_loss: 0.2086\n",
            "Epoch 116 | train_loss: 7.8369 | val_loss: 0.2117\n",
            "Epoch 117 | train_loss: 7.9257 | val_loss: 0.2193\n",
            "Epoch 118 | train_loss: 7.9447 | val_loss: 0.2068\n",
            "Epoch 119 | train_loss: 7.8935 | val_loss: 0.2204\n",
            "Epoch 120 | train_loss: 7.8487 | val_loss: 0.2051\n",
            "Epoch 121 | train_loss: 8.3117 | val_loss: 0.2067\n",
            "Epoch 122 | train_loss: 7.9114 | val_loss: 0.2138\n",
            "Epoch 123 | train_loss: 7.9371 | val_loss: 0.2149\n",
            "Epoch 124 | train_loss: 7.8966 | val_loss: 0.2056\n",
            "Epoch 125 | train_loss: 7.6946 | val_loss: 0.2123\n",
            "Epoch 126 | train_loss: 7.7630 | val_loss: 0.2148\n",
            "Epoch 127 | train_loss: 8.0150 | val_loss: 0.2294\n",
            "Epoch 128 | train_loss: 8.3791 | val_loss: 0.2044\n",
            "Epoch 129 | train_loss: 9.1006 | val_loss: 0.2189\n",
            "Epoch 130 | train_loss: 8.0363 | val_loss: 0.2009\n",
            "Epoch 131 | train_loss: 7.9220 | val_loss: 0.2156\n",
            "Epoch 132 | train_loss: 7.9265 | val_loss: 0.2154\n",
            "Epoch 133 | train_loss: 7.6913 | val_loss: 0.2153\n",
            "Epoch 134 | train_loss: 7.8180 | val_loss: 0.2024\n",
            "Epoch 135 | train_loss: 8.1731 | val_loss: 0.2198\n",
            "Epoch 136 | train_loss: 7.9705 | val_loss: 0.2239\n",
            "Epoch 137 | train_loss: 8.0940 | val_loss: 0.2035\n",
            "Epoch 138 | train_loss: 8.0676 | val_loss: 0.2200\n",
            "Epoch 139 | train_loss: 8.2968 | val_loss: 0.2297\n",
            "Epoch 140 | train_loss: 9.4658 | val_loss: 0.2204\n",
            "Epoch 141 | train_loss: 8.3961 | val_loss: 0.2096\n",
            "Epoch 142 | train_loss: 7.9757 | val_loss: 0.2062\n",
            "Epoch 143 | train_loss: 7.7666 | val_loss: 0.2240\n",
            "Epoch 144 | train_loss: 7.8323 | val_loss: 0.2058\n",
            "Epoch 145 | train_loss: 7.7078 | val_loss: 0.2144\n",
            "Epoch 146 | train_loss: 7.9234 | val_loss: 0.2149\n",
            "Epoch 147 | train_loss: 7.7862 | val_loss: 0.2102\n",
            "Epoch 148 | train_loss: 7.8022 | val_loss: 0.2168\n",
            "Epoch 149 | train_loss: 7.8545 | val_loss: 0.2091\n",
            "Epoch 150 | train_loss: 7.7994 | val_loss: 0.2146\n",
            "Epoch 151 | train_loss: 7.8337 | val_loss: 0.2171\n",
            "Epoch 152 | train_loss: 7.9868 | val_loss: 0.2086\n",
            "Epoch 153 | train_loss: 7.8263 | val_loss: 0.2060\n",
            "Epoch 154 | train_loss: 7.9893 | val_loss: 0.2428\n",
            "Epoch 155 | train_loss: 7.9834 | val_loss: 0.2146\n",
            "Epoch 156 | train_loss: 7.6849 | val_loss: 0.2196\n",
            "Epoch 157 | train_loss: 7.8380 | val_loss: 0.2116\n",
            "Epoch 158 | train_loss: 7.5771 | val_loss: 0.2094\n",
            "Epoch 159 | train_loss: 7.7490 | val_loss: 0.2037\n",
            "Epoch 160 | train_loss: 7.9983 | val_loss: 0.2071\n",
            "Epoch 161 | train_loss: 7.8232 | val_loss: 0.2144\n",
            "Epoch 162 | train_loss: 7.7578 | val_loss: 0.2090\n",
            "Epoch 163 | train_loss: 8.2738 | val_loss: 0.2263\n",
            "Epoch 164 | train_loss: 9.6743 | val_loss: 0.2106\n",
            "Epoch 165 | train_loss: 8.5010 | val_loss: 0.2106\n",
            "Epoch 166 | train_loss: 7.9436 | val_loss: 0.2097\n",
            "Epoch 167 | train_loss: 7.7397 | val_loss: 0.2078\n",
            "Epoch 168 | train_loss: 7.9743 | val_loss: 0.2069\n",
            "Epoch 169 | train_loss: 7.8171 | val_loss: 0.2125\n",
            "Epoch 170 | train_loss: 7.6001 | val_loss: 0.2090\n",
            "Epoch 171 | train_loss: 7.7089 | val_loss: 0.2432\n",
            "Epoch 172 | train_loss: 8.3048 | val_loss: 0.2103\n",
            "Epoch 173 | train_loss: 7.9529 | val_loss: 0.2127\n",
            "Epoch 174 | train_loss: 7.7193 | val_loss: 0.2088\n",
            "Epoch 175 | train_loss: 7.6629 | val_loss: 0.2139\n",
            "Epoch 176 | train_loss: 7.7269 | val_loss: 0.2164\n",
            "Epoch 177 | train_loss: 7.6779 | val_loss: 0.2538\n",
            "Epoch 178 | train_loss: 7.9336 | val_loss: 0.2082\n",
            "Epoch 179 | train_loss: 8.3124 | val_loss: 0.2182\n",
            "Epoch 180 | train_loss: 7.7651 | val_loss: 0.2195\n",
            "Epoch 181 | train_loss: 7.6545 | val_loss: 0.2052\n",
            "Epoch 182 | train_loss: 7.9013 | val_loss: 0.2076\n",
            "Epoch 183 | train_loss: 7.9731 | val_loss: 0.2084\n",
            "Epoch 184 | train_loss: 7.9130 | val_loss: 0.2158\n",
            "Epoch 185 | train_loss: 7.6906 | val_loss: 0.2120\n",
            "Epoch 186 | train_loss: 7.6622 | val_loss: 0.2076\n",
            "Epoch 187 | train_loss: 11.3420 | val_loss: 0.2083\n",
            "Epoch 188 | train_loss: 8.6182 | val_loss: 0.2246\n",
            "Epoch 189 | train_loss: 8.2110 | val_loss: 0.2150\n",
            "Epoch 190 | train_loss: 7.9427 | val_loss: 0.2059\n",
            "Epoch 191 | train_loss: 8.1059 | val_loss: 0.2105\n",
            "Epoch 192 | train_loss: 7.9129 | val_loss: 0.2091\n",
            "Epoch 193 | train_loss: 7.7762 | val_loss: 0.2143\n",
            "Epoch 194 | train_loss: 7.7056 | val_loss: 0.2068\n",
            "Epoch 195 | train_loss: 7.5572 | val_loss: 0.2181\n",
            "Epoch 196 | train_loss: 7.7034 | val_loss: 0.2056\n",
            "Epoch 197 | train_loss: 7.4955 | val_loss: 0.2081\n",
            "Epoch 198 | train_loss: 7.6378 | val_loss: 0.2048\n",
            "Epoch 199 | train_loss: 8.1058 | val_loss: 0.2143\n",
            "Epoch 200 | train_loss: 7.4758 | val_loss: 0.2179\n",
            "\n",
            "Test RMSE: 49665.945\n",
            "Actual: 200624, Predicted: 251953\n",
            "Actual: 133000, Predicted: 147315\n",
            "Actual: 110000, Predicted: 128095\n",
            "Actual: 192000, Predicted: 203143\n",
            "Actual: 88000, Predicted: 136530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load and prepare dataset\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Flatten images and normalize to [0,1]\n",
        "X_train = X_train.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "X_test = X_test.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train = encoder.fit_transform(y_train.reshape(-1,1)).astype(np.float32)\n",
        "y_test = encoder.transform(y_test.reshape(-1,1)).astype(np.float32)\n",
        "\n",
        "# Split train into train / val\n",
        "val_size = 5000\n",
        "X_val = X_train[-val_size:]\n",
        "y_val = y_train[-val_size:]\n",
        "X_train = X_train[:-val_size]\n",
        "y_train = y_train[:-val_size]\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "n_input = X_train.shape[1]\n",
        "n_hidden1 = 256\n",
        "n_hidden2 = 128\n",
        "n_classes = 10\n",
        "\n",
        "# Build model (manual weights)\n",
        "initializer = tf.initializers.GlorotUniform(seed=0)\n",
        "\n",
        "W1 = tf.Variable(initializer([n_input, n_hidden1]))\n",
        "b1 = tf.Variable(tf.zeros([n_hidden1]))\n",
        "W2 = tf.Variable(initializer([n_hidden1, n_hidden2]))\n",
        "b2 = tf.Variable(tf.zeros([n_hidden2]))\n",
        "W3 = tf.Variable(initializer([n_hidden2, n_classes]))\n",
        "b3 = tf.Variable(tf.zeros([n_classes]))\n",
        "\n",
        "def forward(x):\n",
        "    layer1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
        "    logits = tf.matmul(layer2, W3) + b3\n",
        "    return logits\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# Mini-batch generator\n",
        "def get_mini_batches(X, y, batch_size):\n",
        "    n = X.shape[0]\n",
        "    idx = np.random.permutation(n)\n",
        "    X, y = X[idx], y[idx]\n",
        "    for i in range(0, n, batch_size):\n",
        "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for X_batch, y_batch in get_mini_batches(X_train, y_train, batch_size):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = forward(X_batch)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
        "        optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3]))\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "    # Validation\n",
        "    val_logits = forward(X_val)\n",
        "    val_loss = loss_fn(y_val, val_logits).numpy()\n",
        "    val_pred = tf.argmax(tf.nn.softmax(val_logits), axis=1)\n",
        "    val_true = tf.argmax(y_val, axis=1)\n",
        "    val_acc = np.mean((val_pred.numpy() == val_true.numpy()).astype(np.float32))\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | train_loss: {total_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.3f}\")\n",
        "\n",
        "# Evaluate on test data\n",
        "test_logits = forward(X_test)\n",
        "test_pred = tf.argmax(tf.nn.softmax(test_logits), axis=1)\n",
        "test_true = tf.argmax(y_test, axis=1)\n",
        "test_acc = np.mean((test_pred.numpy() == test_true.numpy()).astype(np.float32))\n",
        "print(\"\\nTest Accuracy:\", test_acc)\n",
        "\n",
        "# Example predictions\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {test_true[i].numpy()}, Predicted: {test_pred[i].numpy()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWLgx5wZLATY",
        "outputId": "5263688c-c78a-438c-c871-d2008389737e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss: 203.2308 | val_loss: 0.1020 | val_acc: 0.969\n",
            "Epoch 02 | train_loss: 79.1971 | val_loss: 0.0756 | val_acc: 0.978\n",
            "Epoch 03 | train_loss: 51.7607 | val_loss: 0.0702 | val_acc: 0.981\n",
            "Epoch 04 | train_loss: 38.4979 | val_loss: 0.0755 | val_acc: 0.981\n",
            "Epoch 05 | train_loss: 27.8372 | val_loss: 0.0779 | val_acc: 0.980\n",
            "Epoch 06 | train_loss: 26.6687 | val_loss: 0.1097 | val_acc: 0.970\n",
            "Epoch 07 | train_loss: 18.4429 | val_loss: 0.0976 | val_acc: 0.976\n",
            "Epoch 08 | train_loss: 15.6821 | val_loss: 0.0773 | val_acc: 0.982\n",
            "Epoch 09 | train_loss: 16.0049 | val_loss: 0.0927 | val_acc: 0.978\n",
            "Epoch 10 | train_loss: 12.5732 | val_loss: 0.0894 | val_acc: 0.980\n",
            "Epoch 11 | train_loss: 11.8312 | val_loss: 0.0925 | val_acc: 0.980\n",
            "Epoch 12 | train_loss: 10.4327 | val_loss: 0.1087 | val_acc: 0.981\n",
            "Epoch 13 | train_loss: 11.0202 | val_loss: 0.0887 | val_acc: 0.980\n",
            "Epoch 14 | train_loss: 9.5007 | val_loss: 0.0941 | val_acc: 0.984\n",
            "Epoch 15 | train_loss: 7.5043 | val_loss: 0.1085 | val_acc: 0.981\n",
            "Epoch 16 | train_loss: 8.2416 | val_loss: 0.1144 | val_acc: 0.979\n",
            "Epoch 17 | train_loss: 8.2691 | val_loss: 0.1284 | val_acc: 0.978\n",
            "Epoch 18 | train_loss: 7.4243 | val_loss: 0.1154 | val_acc: 0.982\n",
            "Epoch 19 | train_loss: 7.8617 | val_loss: 0.1013 | val_acc: 0.984\n",
            "Epoch 20 | train_loss: 6.5515 | val_loss: 0.1072 | val_acc: 0.983\n",
            "\n",
            "Test Accuracy: 0.9822\n",
            "Actual: 7, Predicted: 7\n",
            "Actual: 2, Predicted: 2\n",
            "Actual: 1, Predicted: 1\n",
            "Actual: 0, Predicted: 0\n",
            "Actual: 4, Predicted: 4\n"
          ]
        }
      ]
    }
  ]
}